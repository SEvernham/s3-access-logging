AWSTemplateFormatVersion: '2010-09-09'
Description: 'S3 Bucket Access Logging with CloudTrail, CloudWatch, and Weekly S3 Archives'

Conditions:
  HasArchiveBucketName: !Not [!Equals [!Ref ArchiveBucketName, ""]]

Parameters:
  S3BucketName:
    Type: String
    Description: Name of the S3 bucket to monitor
    Default: my-monitored-bucket
  
  LogRetentionDays:
    Type: Number
    Description: Number of days to retain logs
    Default: 7
    MinValue: 1
    MaxValue: 365
  
  ArchiveBucketName:
    Type: String
    Description: Name of the S3 bucket for weekly log archives (will be created)
    Default: ""

Resources:
  # S3 Bucket for weekly log archives
  WeeklyLogArchiveBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 
        - '${BucketName}'
        - BucketName: !If 
          - HasArchiveBucketName
          - !Ref ArchiveBucketName
          - !Sub '${S3BucketName}-weekly-logs-${AWS::AccountId}'
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldWeeklyLogs
            Status: Enabled
            ExpirationInDays: 365  # Keep weekly archives for 1 year
          - Id: DeleteIncompleteMultipartUploads
            Status: Enabled
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7

  # S3 Bucket for CloudTrail logs
  CloudTrailLogsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${S3BucketName}-cloudtrail-logs-${AWS::AccountId}'
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldLogs
            Status: Enabled
            ExpirationInDays: !Ref LogRetentionDays

  # S3 Bucket Policy for CloudTrail
  CloudTrailLogsBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref CloudTrailLogsBucket
      PolicyDocument:
        Statement:
          - Sid: AWSCloudTrailAclCheck
            Effect: Allow
            Principal:
              Service: cloudtrail.amazonaws.com
            Action: s3:GetBucketAcl
            Resource: !GetAtt CloudTrailLogsBucket.Arn
          - Sid: AWSCloudTrailWrite
            Effect: Allow
            Principal:
              Service: cloudtrail.amazonaws.com
            Action: s3:PutObject
            Resource: !Sub '${CloudTrailLogsBucket.Arn}/*'
            Condition:
              StringEquals:
                's3:x-amz-acl': bucket-owner-full-control

  # CloudWatch Log Group for formatted logs
  S3AccessLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/s3/${S3BucketName}/access-logs'
      RetentionInDays: !Ref LogRetentionDays

  # IAM Role for CloudTrail
  CloudTrailRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service: cloudtrail.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: CloudTrailLogsPolicy
          PolicyDocument:
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !GetAtt S3AccessLogGroup.Arn

  # CloudTrail for S3 API logging
  S3CloudTrail:
    Type: AWS::CloudTrail::Trail
    DependsOn: CloudTrailLogsBucketPolicy
    Properties:
      TrailName: !Sub '${S3BucketName}-access-trail'
      S3BucketName: !Ref CloudTrailLogsBucket
      S3KeyPrefix: 's3-access-logs'
      IncludeGlobalServiceEvents: false
      IsMultiRegionTrail: false
      EnableLogFileValidation: true
      EventSelectors:
        - ReadWriteType: All
          IncludeManagementEvents: false
          DataResources:
            - Type: AWS::S3::Object
              Values:
                - !Sub 'arn:aws:s3:::${S3BucketName}/*'
            - Type: AWS::S3::Bucket
              Values:
                - !Sub 'arn:aws:s3:::${S3BucketName}'

  # Lambda execution role
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: CloudWatchLogsPolicy
          PolicyDocument:
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !GetAtt S3AccessLogGroup.Arn
        - PolicyName: S3ReadPolicy
          PolicyDocument:
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                Resource: !Sub '${CloudTrailLogsBucket}/*'
        - PolicyName: S3ArchivePolicy
          PolicyDocument:
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                Resource: !Sub '${WeeklyLogArchiveBucket}/*'
              - Effect: Allow
                Action:
                  - s3:ListBucket
                Resource: !GetAtt WeeklyLogArchiveBucket.Arn

  # Lambda function to process CloudTrail logs
  LogProcessorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${S3BucketName}-log-processor'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 300
      Environment:
        Variables:
          LOG_GROUP_NAME: !Ref S3AccessLogGroup
          MONITORED_BUCKET: !Ref S3BucketName
          ARCHIVE_BUCKET: !Ref WeeklyLogArchiveBucket
      Code:
        ZipFile: |
          import json
          import boto3
          import gzip
          import base64
          import os
          from datetime import datetime, timedelta
          from collections import defaultdict
          
          logs_client = boto3.client('logs')
          s3_client = boto3.client('s3')
          
          def lambda_handler(event, context):
              log_group_name = os.environ['LOG_GROUP_NAME']
              monitored_bucket = os.environ['MONITORED_BUCKET']
              archive_bucket = os.environ['ARCHIVE_BUCKET']
              
              for record in event['Records']:
                  # Decode CloudTrail log
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  
                  try:
                      # Get the CloudTrail log file
                      response = s3_client.get_object(Bucket=bucket, Key=key)
                      
                      # Decompress if gzipped
                      if key.endswith('.gz'):
                          content = gzip.decompress(response['Body'].read())
                      else:
                          content = response['Body'].read()
                      
                      # Parse CloudTrail records
                      log_data = json.loads(content)
                      
                      # Collect logs for weekly archive
                      weekly_logs = []
                      
                      for ct_record in log_data.get('Records', []):
                          # Filter for our monitored bucket
                          if is_s3_event_for_bucket(ct_record, monitored_bucket):
                              formatted_log = format_s3_access_log(ct_record)
                              
                              # Send to CloudWatch
                              send_to_cloudwatch(log_group_name, formatted_log)
                              
                              # Collect for weekly archive
                              weekly_logs.append(json.loads(formatted_log))
                      
                      # Archive to S3 weekly bucket
                      if weekly_logs:
                          archive_weekly_logs(archive_bucket, weekly_logs)
                              
                  except Exception as e:
                      print(f"Error processing {key}: {str(e)}")
              
              return {'statusCode': 200}
          
          def is_s3_event_for_bucket(record, bucket_name):
              """Check if the CloudTrail record is for our monitored S3 bucket"""
              if record.get('eventSource') != 's3.amazonaws.com':
                  return False
              
              resources = record.get('resources', [])
              for resource in resources:
                  arn = resource.get('arn', '')
                  if f'arn:aws:s3:::{bucket_name}' in arn:
                      return True
              return False
          
          def format_s3_access_log(record):
              """Format CloudTrail record into readable S3 access log"""
              event_time = record.get('eventTime', '')
              event_name = record.get('eventName', '')
              user_identity = record.get('userIdentity', {})
              source_ip = record.get('sourceIPAddress', '')
              user_agent = record.get('userAgent', '')
              request_params = record.get('requestParameters', {})
              resources = record.get('resources', [])
              
              # Extract user information
              user_type = user_identity.get('type', 'Unknown')
              user_name = user_identity.get('userName', 
                         user_identity.get('principalId', 
                         user_identity.get('arn', 'Unknown')))
              
              # Extract resource information
              resource_info = []
              for resource in resources:
                  resource_info.append(resource.get('arn', ''))
              
              # Map event names to CRUD operations
              operation_map = {
                  'GetObject': 'READ',
                  'PutObject': 'CREATE/UPDATE',
                  'DeleteObject': 'DELETE',
                  'CopyObject': 'CREATE/UPDATE',
                  'RestoreObject': 'UPDATE',
                  'ListBucket': 'READ',
                  'GetBucketLocation': 'READ',
                  'GetBucketVersioning': 'READ',
                  'PutBucketVersioning': 'UPDATE',
                  'DeleteBucket': 'DELETE'
              }
              
              operation = operation_map.get(event_name, event_name)
              
              # Format the log entry
              log_entry = {
                  'timestamp': event_time,
                  'operation': operation,
                  'event_name': event_name,
                  'who': {
                      'user_type': user_type,
                      'user_name': user_name,
                      'source_ip': source_ip
                  },
                  'what': {
                      'resources': resource_info,
                      'bucket': request_params.get('bucketName', ''),
                      'key': request_params.get('key', '')
                  },
                  'how': {
                      'user_agent': user_agent,
                      'request_id': record.get('requestID', ''),
                      'aws_region': record.get('awsRegion', '')
                  },
                  'response': {
                      'error_code': record.get('errorCode', ''),
                      'error_message': record.get('errorMessage', '')
                  }
              }
              
              return json.dumps(log_entry, separators=(',', ':'))
          
          def send_to_cloudwatch(log_group_name, message):
              """Send formatted log to CloudWatch"""
              try:
                  # Create log stream if it doesn't exist
                  stream_name = datetime.now().strftime('%Y/%m/%d/s3-access')
                  
                  try:
                      logs_client.create_log_stream(
                          logGroupName=log_group_name,
                          logStreamName=stream_name
                      )
                  except logs_client.exceptions.ResourceAlreadyExistsException:
                      pass
                  
                  # Send log event
                  logs_client.put_log_events(
                      logGroupName=log_group_name,
                      logStreamName=stream_name,
                      logEvents=[
                          {
                              'timestamp': int(datetime.now().timestamp() * 1000),
                              'message': message
                          }
                      ]
                  )
              except Exception as e:
                  print(f"Error sending to CloudWatch: {str(e)}")
          
          def archive_weekly_logs(archive_bucket, log_entries):
              """Archive logs to S3 bucket with weekly aggregation"""
              try:
                  # Group logs by week
                  weekly_groups = defaultdict(list)
                  
                  for log_entry in log_entries:
                      timestamp = log_entry.get('timestamp', '')
                      if timestamp:
                          try:
                              # Parse timestamp and get week start
                              dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                              # Get Monday of the week (ISO week)
                              week_start = dt - timedelta(days=dt.weekday())
                              week_key = week_start.strftime('%Y-W%U')  # Year-Week format
                              weekly_groups[week_key].append(log_entry)
                          except Exception as e:
                              print(f"Error parsing timestamp {timestamp}: {e}")
                              # Use current week as fallback
                              current_week = datetime.now().strftime('%Y-W%U')
                              weekly_groups[current_week].append(log_entry)
                  
                  # Process each week's logs
                  for week_key, week_logs in weekly_groups.items():
                      s3_key = f"weekly-logs/{week_key}.json"
                      
                      # Try to get existing weekly log file
                      existing_logs = []
                      try:
                          response = s3_client.get_object(Bucket=archive_bucket, Key=s3_key)
                          existing_content = response['Body'].read().decode('utf-8')
                          existing_data = json.loads(existing_content)
                          existing_logs = existing_data.get('logs', [])
                      except s3_client.exceptions.NoSuchKey:
                          # File doesn't exist yet, start fresh
                          pass
                      except Exception as e:
                          print(f"Error reading existing weekly log {s3_key}: {e}")
                      
                      # Combine existing and new logs
                      all_logs = existing_logs + week_logs
                      
                      # Create weekly archive structure
                      weekly_archive = {
                          'week': week_key,
                          'generated_at': datetime.now().isoformat(),
                          'total_events': len(all_logs),
                          'summary': generate_weekly_summary(all_logs),
                          'logs': all_logs
                      }
                      
                      # Upload to S3
                      s3_client.put_object(
                          Bucket=archive_bucket,
                          Key=s3_key,
                          Body=json.dumps(weekly_archive, indent=2, default=str),
                          ContentType='application/json',
                          Metadata={
                              'week': week_key,
                              'total_events': str(len(all_logs)),
                              'last_updated': datetime.now().isoformat()
                          }
                      )
                      
                      print(f"Archived {len(week_logs)} new logs to {s3_key} (total: {len(all_logs)})")
                      
              except Exception as e:
                  print(f"Error archiving weekly logs: {str(e)}")
          
          def generate_weekly_summary(logs):
              """Generate summary statistics for weekly logs"""
              from collections import Counter
              
              operations = Counter()
              users = Counter()
              ips = Counter()
              errors = 0
              
              for log in logs:
                  # Count operations
                  operation = log.get('operation', 'Unknown')
                  operations[operation] += 1
                  
                  # Count users
                  user = log.get('who', {}).get('user_name', 'Unknown')
                  users[user] += 1
                  
                  # Count IPs
                  ip = log.get('who', {}).get('source_ip', 'Unknown')
                  ips[ip] += 1
                  
                  # Count errors
                  if log.get('response', {}).get('error_code'):
                      errors += 1
              
              return {
                  'total_events': len(logs),
                  'error_count': errors,
                  'top_operations': dict(operations.most_common(10)),
                  'top_users': dict(users.most_common(10)),
                  'top_source_ips': dict(ips.most_common(10)),
                  'unique_users': len(users),
                  'unique_ips': len(ips)
              }

  # Lambda permission for S3 to invoke the function
  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref LogProcessorFunction
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceArn: !GetAtt CloudTrailLogsBucket.Arn

  # S3 notification configuration
  S3NotificationConfiguration:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref CloudTrailLogsBucket
      NotificationConfiguration:
        LambdaConfigurations:
          - Event: s3:ObjectCreated:*
            Function: !GetAtt LogProcessorFunction.Arn
            Filter:
              S3Key:
                Rules:
                  - Name: prefix
                    Value: s3-access-logs/

Outputs:
  CloudTrailArn:
    Description: ARN of the CloudTrail for S3 access logging
    Value: !GetAtt S3CloudTrail.Arn
    
  LogGroupName:
    Description: CloudWatch Log Group for formatted S3 access logs
    Value: !Ref S3AccessLogGroup
    
  LogGroupArn:
    Description: ARN of the CloudWatch Log Group
    Value: !GetAtt S3AccessLogGroup.Arn
    
  CloudTrailBucket:
    Description: S3 bucket storing raw CloudTrail logs
    Value: !Ref CloudTrailLogsBucket
    
  WeeklyArchiveBucket:
    Description: S3 bucket storing weekly log archives
    Value: !Ref WeeklyLogArchiveBucket
    
  LogProcessorFunction:
    Description: Lambda function processing CloudTrail logs
    Value: !Ref LogProcessorFunction
    
  ViewLogsCommand:
    Description: AWS CLI command to view the formatted logs
    Value: !Sub 'aws logs filter-log-events --log-group-name ${S3AccessLogGroup} --start-time $(date -d "1 hour ago" +%s)000'
    
  ViewWeeklyArchivesCommand:
    Description: AWS CLI command to list weekly log archives
    Value: !Sub 'aws s3 ls s3://${WeeklyLogArchiveBucket}/weekly-logs/'
    
  DownloadWeeklyLogCommand:
    Description: AWS CLI command to download a weekly log file (replace YYYY-WWW with actual week)
    Value: !Sub 'aws s3 cp s3://${WeeklyLogArchiveBucket}/weekly-logs/YYYY-WWW.json .'
